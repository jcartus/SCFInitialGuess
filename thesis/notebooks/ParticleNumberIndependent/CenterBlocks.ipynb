{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import seaborn as sns\n",
    "\n",
    "from os.path import join\n",
    "\n",
    "plt.style.use([\"seaborn\", \"thesis\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SCFInitialGuess.utilities.dataset import extract_triu_batch, AbstractDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_path = \"../../dataset/TSmall_sto3g\"\n",
    "postfix = \"TSmall_sto3g\"\n",
    "dim = 26\n",
    "data_path = \"../../dataset/EthenT/\"\n",
    "postfix = \"EthenT\"\n",
    "dim = 72\n",
    "basis = \"6-311++g**\"\n",
    "n_electrons = 16\n",
    "#data_path = \"../butadien/data/\"\n",
    "#postfix = \"\"\n",
    "#dim = 26\n",
    "\n",
    "\n",
    "def split(x, y, ind):\n",
    "    return x[:ind], y[:ind], x[ind:], y[ind:]\n",
    "\n",
    "S = np.load(join(data_path, \"S\" + postfix + \".npy\"))\n",
    "P = np.load(join(data_path, \"P\" + postfix + \".npy\"))\n",
    "\n",
    "index = np.load(join(data_path, \"index\" + postfix + \".npy\"))\n",
    "\n",
    "molecules = np.load(join(data_path, \"molecules\" + postfix + \".npy\"))\n",
    "\n",
    "\n",
    "ind = int(0.8 * len(index))\n",
    "ind_val = int(0.8 * ind)\n",
    "\n",
    "\n",
    "molecules = (\n",
    "    molecules[:ind_val], \n",
    "    molecules[ind_val:ind], \n",
    "    molecules[ind:]\n",
    ")\n",
    "\n",
    "s_triu_norm, mu, std = AbstractDataset.normalize(S)\n",
    "\n",
    "\n",
    "s_train, p_train, s_test, p_test = split(S, P, ind)\n",
    "s_train, p_train, s_val, p_val = split(s_train, p_train, ind_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Descriptors and extract center blocks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SCFInitialGuess.utilities.constants import number_of_basis_functions as N_BASIS\n",
    "from SCFInitialGuess.descriptors.coordinate_descriptors import \\\n",
    "    AtomicNumberWeighted, Gaussians\n",
    "from SCFInitialGuess.descriptors.coordinate_descriptors import \\\n",
    "    Gaussians, RADIAL_GAUSSIAN_MODELS, AZIMUTHAL_GAUSSIAN_MODELS, POLAR_GAUSSIAN_MODELS\n",
    "from SCFInitialGuess.utilities.dataset import extract_triu\n",
    "from SCFInitialGuess.utilities.dataset import StaticDataset\n",
    "\n",
    "\n",
    "def make_mask(mol, species):\n",
    "\n",
    "    masks = []\n",
    "    current_dim = 0\n",
    "    for atom in mol.species:\n",
    "        # calculate block range\n",
    "        index_start = current_dim\n",
    "        current_dim += N_BASIS[mol.basis][atom] \n",
    "        index_end = current_dim\n",
    "\n",
    "        if atom == species:\n",
    "\n",
    "            # calculate logical vector\n",
    "            L = np.arange(dim)\n",
    "            L = np.logical_and(index_start <= L, L < index_end)\n",
    "\n",
    "            masks.append(np.logical_and.outer(L, L))\n",
    "            \n",
    "    \n",
    "    return masks\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_dataset(molecules, p_batch, species):    \n",
    "    \n",
    "    # make mask to extract central blocks\n",
    "    masks = make_mask(molecules[0], species)\n",
    "    \n",
    "    descriptor = AtomicNumberWeighted(\n",
    "        Gaussians(*RADIAL_GAUSSIAN_MODELS[\"Equidistant-Broadening_1\"]),\n",
    "        Gaussians(*AZIMUTHAL_GAUSSIAN_MODELS[\"Equisitant_1\"]),\n",
    "        Gaussians(*POLAR_GAUSSIAN_MODELS[\"Equisitant_1\"])\n",
    "    )\n",
    "    \n",
    "    descriptor_values, blocks = [], []\n",
    "    for p, mol in zip(p_batch, molecules):\n",
    "        for mask in masks:\n",
    "            blocks.append(extract_triu(\n",
    "                p.copy()[mask], \n",
    "                N_BASIS[mol.basis][species]\n",
    "            ))\n",
    "        \n",
    "        for i, atom in enumerate(mol.species):\n",
    "            if atom == species:\n",
    "                descriptor_values.append(\n",
    "                    descriptor.calculate_atom_descriptor(\n",
    "                        i, \n",
    "                        mol,\n",
    "                        descriptor.number_of_descriptors\n",
    "                    )\n",
    "                )\n",
    "            \n",
    "    return descriptor_values, blocks\n",
    "\n",
    "\n",
    "def make_dataset(species):\n",
    "    \n",
    "    inputs_test, outputs_test = extract_dataset(\n",
    "        molecules[2], \n",
    "        p_test.reshape(-1, dim, dim),\n",
    "        species\n",
    "    )\n",
    "    \n",
    "    inputs_validation, outputs_validation = extract_dataset(\n",
    "        molecules[1], \n",
    "        p_val.reshape(-1, dim, dim),\n",
    "        species\n",
    "    )\n",
    "\n",
    "    inputs_train, outputs_train = extract_dataset(\n",
    "        molecules[0], \n",
    "        p_train.reshape(-1, dim, dim),\n",
    "        species\n",
    "    )\n",
    "    \n",
    "    \n",
    "    _, mu, std = StaticDataset.normalize(inputs_train + inputs_validation + inputs_test)\n",
    "    \n",
    "    dataset = StaticDataset(\n",
    "        train=(\n",
    "            StaticDataset.normalize(inputs_train, mu, std)[0], \n",
    "            np.asarray(outputs_train)\n",
    "        ),\n",
    "        validation=(\n",
    "            StaticDataset.normalize(inputs_validation, mu, std)[0], \n",
    "            np.asarray(outputs_validation)\n",
    "        ),\n",
    "        test=(\n",
    "            StaticDataset.normalize(inputs_test, mu, std)[0], \n",
    "            np.asarray(outputs_test)\n",
    "        ),\n",
    "        mu=mu,\n",
    "        std=std\n",
    "    )\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Networks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keras.backend.clear_session()\n",
    "\n",
    "#activation = \"elu\"\n",
    "#learning_rate = 1e-5\n",
    "\n",
    "intializer = keras.initializers.TruncatedNormal(mean=0.0, stddev=0.01)\n",
    "\n",
    "def make_model(\n",
    "        structure, \n",
    "        input_dim, \n",
    "        output_dim,\n",
    "        activation=\"elu\", \n",
    "        learning_rate=1e-5\n",
    "    ):\n",
    "\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    # input layer\n",
    "    model.add(keras.layers.Dense(\n",
    "        structure[0], \n",
    "        activation=activation, \n",
    "        input_dim=input_dim, \n",
    "        kernel_initializer=intializer\n",
    "    ))\n",
    "\n",
    "    for layer in structure[1:]:\n",
    "        model.add(keras.layers.Dense(\n",
    "            layer, \n",
    "            activation=activation, \n",
    "            kernel_initializer=intializer, \n",
    "            #bias_initializer='zeros',\n",
    "            #kernel_regularizer=keras.regularizers.l2(1e-8)\n",
    "        ))\n",
    "\n",
    "    #output\n",
    "    model.add(keras.layers.Dense(output_dim))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate), \n",
    "        loss='MSE', \n",
    "        metrics=['mae', 'mse']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"../../models/ParticleNumberIndependent/CenterBlocks/model_C_\" + postfix + \".h5\"\n",
    "\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_mean_squared_error\", \n",
    "    min_delta=1e-7, \n",
    "    patience=200, \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', \n",
    "    factor=0.5, \n",
    "    patience=50, \n",
    "    verbose=1, \n",
    "    mode='auto', \n",
    "    min_delta=1e-4, \n",
    "    cooldown=50, \n",
    "    min_lr=1e-10\n",
    ")\n",
    "\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "    filepath, \n",
    "    monitor='val_loss', \n",
    "    verbose=1, \n",
    "    save_best_only=False, \n",
    "    save_weights_only=False, \n",
    "    mode='auto', \n",
    "    period=1\n",
    ")\n",
    "\n",
    "\n",
    "def train_model(model, dataset):\n",
    "    \n",
    "    error = []\n",
    "    while True:\n",
    "        keras.backend.set_value(model.optimizer.lr, learning_rate)\n",
    "            \n",
    "        history = model.fit(\n",
    "            x = dataset.training[0],\n",
    "            y = dataset.training[1],\n",
    "            epochs=epochs,\n",
    "            shuffle=True,\n",
    "            validation_data=dataset.validation, \n",
    "            verbose=0, \n",
    "            callbacks=[\n",
    "                early_stopping, \n",
    "                reduce_lr\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        error.append(model.evaluate(\n",
    "            dataset.testing[0], \n",
    "            dataset.testing[1], \n",
    "            verbose=0\n",
    "        )[1])\n",
    "    \n",
    "    return error\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SCFInitialGuess.utilities.constants import number_of_basis_functions as N_BASIS\n",
    "\n",
    "dim_C = N_BASIS[basis][species]\n",
    "dim_C_triu = dim_C * (dim_C + 1) // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "species = \"C\"\n",
    "dataset_C = make_dataset(species)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "structure_C = [dim_C_triu + 200, dim_C_triu + 100, dim_C_triu + 50]\n",
    "\n",
    "model_C = make_model(\n",
    "    structure=structure_C,\n",
    "    input_dim=dataset_C.training[0].shape[1],\n",
    "    output_dim=dim_C_triu\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_resets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-0e6bb087ddb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_C\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_C\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-29-45b6428a29f8>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, dataset)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_resets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'n_resets' is not defined"
     ]
    }
   ],
   "source": [
    "train_model(model_C, dataset_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
