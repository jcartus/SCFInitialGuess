{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "S, P = np.load(\"butadien/dataset.npy\")\n",
    "molecules = np.load(\"butadien/molecules.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-] 2018-03-23 18:01:24: Data set normalized. Mean value std: 0.00869629472968855\n"
     ]
    }
   ],
   "source": [
    "from SCFInitialGuess.utilities.dataset import Dataset\n",
    "\n",
    "dim = 26\n",
    "\n",
    "ind_cut = 150\n",
    "index = np.arange(200)\n",
    "np.random.shuffle(index)\n",
    "\n",
    "S_test = np.array(S)[index[150:]]\n",
    "P_test = np.array(P)[index[150:]]\n",
    "molecules_test = [molecules[index[i]] for i in range(150, 200)]\n",
    "\n",
    "S_train = np.array(S)[index[:150]]\n",
    "P_train = np.array(P)[index[:150]]\n",
    "molecules_train = [molecules[index[i]] for i in range(150)]\n",
    "\n",
    "dataset = Dataset(np.array(S_train), np.array(P_train), split_test=0.0)\n",
    "\n",
    "dataset.testing = (Dataset.normalize(S_test, mean=dataset.x_mean, std=dataset.x_std)[0], P_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-] 2018-03-23 18:01:25: No target graph specified for Trainer setup. Creating new graph ...\n",
      "[-] 2018-03-23 18:01:25: Setting up the training in the target graph ...\n",
      "[-] 2018-03-23 18:01:25: network ...\n",
      "[-] 2018-03-23 18:01:25: error function ...\n",
      "[-] 2018-03-23 18:01:25: cost function ...\n",
      "[-] 2018-03-23 18:01:34: training step\n",
      "[-] 2018-03-23 18:01:34: Starting network training ...\n",
      "[ ] 2018-03-23 18:01:34: Val. Cost: 5.775E-02. Error: 5.774E-02. Diff: 1.0E+10\n",
      "[ ] 2018-03-23 18:01:38: Val. Cost: 5.082E-02. Error: 5.082E-02. Diff: 6.9E-03\n",
      "[ ] 2018-03-23 18:01:43: Val. Cost: 5.050E-03. Error: 5.042E-03. Diff: 4.6E-02\n",
      "[ ] 2018-03-23 18:01:47: Val. Cost: 6.250E-04. Error: 6.174E-04. Diff: 4.4E-03\n",
      "[ ] 2018-03-23 18:01:51: Val. Cost: 4.969E-04. Error: 4.892E-04. Diff: 1.3E-04\n",
      "[ ] 2018-03-23 18:01:55: Val. Cost: 4.658E-04. Error: 4.581E-04. Diff: 3.1E-05\n",
      "[ ] 2018-03-23 18:01:59: Val. Cost: 4.354E-04. Error: 4.277E-04. Diff: 3.0E-05\n",
      "[ ] 2018-03-23 18:02:04: Val. Cost: 4.344E-04. Error: 4.268E-04. Diff: 9.6E-07\n",
      "[ ] 2018-03-23 18:02:11: Val. Cost: 4.255E-04. Error: 4.179E-04. Diff: 8.9E-06\n",
      "[ ] 2018-03-23 18:02:17: Val. Cost: 4.208E-04. Error: 4.132E-04. Diff: 4.7E-06\n",
      "[ ] 2018-03-23 18:02:24: Val. Cost: 4.219E-04. Error: 4.143E-04. Diff: 1.1E-06\n",
      "[ ] 2018-03-23 18:02:31: Val. Cost: 4.195E-04. Error: 4.119E-04. Diff: 2.4E-06\n",
      "[ ] 2018-03-23 18:02:38: Val. Cost: 4.140E-04. Error: 4.063E-04. Diff: 5.6E-06\n",
      "[ ] 2018-03-23 18:02:44: Val. Cost: 4.149E-04. Error: 4.073E-04. Diff: 9.7E-07\n",
      "[ ] 2018-03-23 18:02:51: Val. Cost: 4.221E-04. Error: 4.145E-04. Diff: 7.2E-06\n",
      "[ ] 2018-03-23 18:02:58: Val. Cost: 4.178E-04. Error: 4.102E-04. Diff: 4.3E-06\n",
      "[ ] 2018-03-23 18:03:05: Val. Cost: 4.113E-04. Error: 4.037E-04. Diff: 6.5E-06\n",
      "[ ] 2018-03-23 18:03:12: Val. Cost: 4.230E-04. Error: 4.154E-04. Diff: 1.2E-05\n",
      "[ ] 2018-03-23 18:03:19: Val. Cost: 4.209E-04. Error: 4.134E-04. Diff: 2.0E-06\n",
      "[ ] 2018-03-23 18:03:25: Val. Cost: 4.166E-04. Error: 4.090E-04. Diff: 4.4E-06\n",
      "[ ] 2018-03-23 18:03:32: Val. Cost: 4.104E-04. Error: 4.028E-04. Diff: 6.2E-06\n",
      "[ ] 2018-03-23 18:03:39: Val. Cost: 4.137E-04. Error: 4.062E-04. Diff: 3.3E-06\n",
      "[ ] 2018-03-23 18:03:46: Val. Cost: 4.151E-04. Error: 4.076E-04. Diff: 1.4E-06\n",
      "[ ] 2018-03-23 18:03:53: Val. Cost: 4.120E-04. Error: 4.045E-04. Diff: 3.1E-06\n",
      "[ ] 2018-03-23 18:04:00: Val. Cost: 4.197E-04. Error: 4.122E-04. Diff: 7.7E-06\n",
      "[ ] 2018-03-23 18:04:06: Val. Cost: 4.173E-04. Error: 4.098E-04. Diff: 2.3E-06\n",
      "[ ] 2018-03-23 18:04:13: Val. Cost: 4.134E-04. Error: 4.059E-04. Diff: 3.9E-06\n",
      "[ ] 2018-03-23 18:04:21: Val. Cost: 4.176E-04. Error: 4.101E-04. Diff: 4.2E-06\n",
      "[ ] 2018-03-23 18:04:28: Val. Cost: 4.225E-04. Error: 4.150E-04. Diff: 4.9E-06\n",
      "[ ] 2018-03-23 18:04:35: Val. Cost: 4.170E-04. Error: 4.095E-04. Diff: 5.5E-06\n",
      "[ ] 2018-03-23 18:04:40: Val. Cost: 4.137E-04. Error: 4.062E-04. Diff: 3.3E-06\n",
      "[ ] 2018-03-23 18:04:44: Val. Cost: 4.249E-04. Error: 4.175E-04. Diff: 1.1E-05\n",
      "[ ] 2018-03-23 18:04:49: Val. Cost: 4.166E-04. Error: 4.092E-04. Diff: 8.2E-06\n",
      "[ ] 2018-03-23 18:04:53: Val. Cost: 4.138E-04. Error: 4.065E-04. Diff: 2.8E-06\n",
      "[ ] 2018-03-23 18:04:57: Val. Cost: 4.187E-04. Error: 4.113E-04. Diff: 4.9E-06\n",
      "[ ] 2018-03-23 18:05:01: Val. Cost: 4.148E-04. Error: 4.075E-04. Diff: 3.9E-06\n",
      "[ ] 2018-03-23 18:05:06: Val. Cost: 4.118E-04. Error: 4.045E-04. Diff: 2.9E-06\n",
      "[ ] 2018-03-23 18:05:10: Val. Cost: 4.157E-04. Error: 4.085E-04. Diff: 3.9E-06\n",
      "[ ] 2018-03-23 18:05:14: Val. Cost: 4.133E-04. Error: 4.061E-04. Diff: 2.4E-06\n",
      "[ ] 2018-03-23 18:05:19: Val. Cost: 4.220E-04. Error: 4.148E-04. Diff: 8.7E-06\n",
      "[ ] 2018-03-23 18:05:23: Val. Cost: 4.115E-04. Error: 4.043E-04. Diff: 1.0E-05\n",
      "[ ] 2018-03-23 18:05:29: Val. Cost: 4.202E-04. Error: 4.130E-04. Diff: 8.7E-06\n",
      "[ ] 2018-03-23 18:05:36: Val. Cost: 4.110E-04. Error: 4.039E-04. Diff: 9.2E-06\n",
      "[ ] 2018-03-23 18:05:43: Val. Cost: 4.122E-04. Error: 4.051E-04. Diff: 1.3E-06\n",
      "[ ] 2018-03-23 18:05:50: Val. Cost: 4.043E-04. Error: 3.972E-04. Diff: 8.0E-06\n",
      "[ ] 2018-03-23 18:05:57: Val. Cost: 3.907E-04. Error: 3.836E-04. Diff: 1.4E-05\n",
      "[ ] 2018-03-23 18:06:03: Val. Cost: 3.561E-04. Error: 3.489E-04. Diff: 3.5E-05\n",
      "[ ] 2018-03-23 18:06:10: Val. Cost: 3.164E-04. Error: 3.092E-04. Diff: 4.0E-05\n",
      "[ ] 2018-03-23 18:06:17: Val. Cost: 2.961E-04. Error: 2.889E-04. Diff: 2.0E-05\n",
      "[ ] 2018-03-23 18:06:24: Val. Cost: 2.783E-04. Error: 2.712E-04. Diff: 1.8E-05\n",
      "[ ] 2018-03-23 18:06:31: Val. Cost: 2.679E-04. Error: 2.607E-04. Diff: 1.0E-05\n",
      "[ ] 2018-03-23 18:06:38: Val. Cost: 2.609E-04. Error: 2.537E-04. Diff: 7.0E-06\n",
      "[ ] 2018-03-23 18:06:46: Val. Cost: 2.492E-04. Error: 2.420E-04. Diff: 1.2E-05\n",
      "[ ] 2018-03-23 18:06:53: Val. Cost: 2.361E-04. Error: 2.289E-04. Diff: 1.3E-05\n",
      "[ ] 2018-03-23 18:07:01: Val. Cost: 2.173E-04. Error: 2.102E-04. Diff: 1.9E-05\n",
      "[ ] 2018-03-23 18:07:08: Val. Cost: 2.055E-04. Error: 1.984E-04. Diff: 1.2E-05\n",
      "[ ] 2018-03-23 18:07:15: Val. Cost: 1.940E-04. Error: 1.869E-04. Diff: 1.1E-05\n",
      "[ ] 2018-03-23 18:07:23: Val. Cost: 1.866E-04. Error: 1.795E-04. Diff: 7.4E-06\n",
      "[ ] 2018-03-23 18:07:30: Val. Cost: 1.822E-04. Error: 1.751E-04. Diff: 4.4E-06\n",
      "[ ] 2018-03-23 18:07:38: Val. Cost: 1.809E-04. Error: 1.739E-04. Diff: 1.2E-06\n",
      "[ ] 2018-03-23 18:07:44: Val. Cost: 1.755E-04. Error: 1.684E-04. Diff: 5.5E-06\n",
      "[ ] 2018-03-23 18:07:51: Val. Cost: 1.681E-04. Error: 1.610E-04. Diff: 7.4E-06\n",
      "[ ] 2018-03-23 18:07:59: Val. Cost: 1.623E-04. Error: 1.552E-04. Diff: 5.8E-06\n",
      "[ ] 2018-03-23 18:08:04: Val. Cost: 1.568E-04. Error: 1.497E-04. Diff: 5.5E-06\n",
      "[ ] 2018-03-23 18:08:08: Val. Cost: 1.500E-04. Error: 1.429E-04. Diff: 6.8E-06\n",
      "[ ] 2018-03-23 18:08:13: Val. Cost: 1.438E-04. Error: 1.366E-04. Diff: 6.3E-06\n",
      "[ ] 2018-03-23 18:08:17: Val. Cost: 1.368E-04. Error: 1.296E-04. Diff: 7.0E-06\n",
      "[ ] 2018-03-23 18:08:22: Val. Cost: 1.255E-04. Error: 1.184E-04. Diff: 1.1E-05\n",
      "[ ] 2018-03-23 18:08:26: Val. Cost: 1.159E-04. Error: 1.087E-04. Diff: 9.6E-06\n"
     ]
    }
   ],
   "source": [
    "from SCFInitialGuess.nn.networks import EluTrNNN\n",
    "from SCFInitialGuess.nn.training import Trainer, RegularizedMSE\n",
    "\n",
    "trainer = Trainer(\n",
    "    EluTrNNN([dim**2, 1000, 600, 400, 200, 100, dim**2]),\n",
    "    cost_function=RegularizedMSE(alpha=1e-7),\n",
    "    optimizer=tf.train.AdamOptimizer(learning_rate=1e-5)\n",
    ")\n",
    "\n",
    "trainer.setup()\n",
    "network_orig, sess_orig = trainer.train(\n",
    "    dataset,\n",
    "    convergence_threshold=1e-7\n",
    ")\n",
    "graph_orig = trainer.graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P'= \\dfrac{3}{2} PSP - \\dfrac{2}{2 \\cdot 2}PSPSP $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mc_wheeny_purification(p,s):\n",
    "    p = p.reshape(dim, dim)\n",
    "    s = s.reshape(dim, dim)\n",
    "    return (3 * np.dot(np.dot(p, s), p) - np.dot(np.dot(np.dot(np.dot(p, s), p), s), p)) / 2\n",
    "\n",
    "def multi_mc_wheeny(p, s, n_max=4):\n",
    "    for i in range(n_max):\n",
    "        p = mc_wheeny_purification(p, s)\n",
    "    return p\n",
    "\n",
    "def idemp_error(p, s):\n",
    "    p = p.reshape(dim, dim)\n",
    "    s = s.reshape(dim, dim)\n",
    "    return np.mean(np.abs(np.dot(np.dot(p, s), p) - 2 * p))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for (s, p) in zip(*dataset.testing):\n",
    "for (s, p) in zip(S_test, P_test):\n",
    "    \n",
    "    #s_norm = s.reshape(1, dim**2)\n",
    "    s_norm = dataset.input_transformation(s.reshape(1, dim**2))\n",
    "    \n",
    "    print(\"Orip:         {:0.3E}\".format(idemp_error(p, s))) \n",
    "    print(\"Orig prurif:  {:0.3E}\".format(idemp_error(mc_wheeny_purification(p, s), s)))\n",
    "    \n",
    "    with graph_orig.as_default():\n",
    "        p_nn = network_orig.run(sess_orig, s_norm).reshape(dim, dim)\n",
    "        \n",
    "    print(\"NN:           {:0.3E}\".format(idemp_error(p_nn, s)))\n",
    "    print(\"NN pruified:  {:0.3E}\".format(idemp_error(mc_wheeny_purification(p_nn, s), s)))\n",
    "    p_nn_multi = multi_mc_wheeny(p_nn, s, n_max=5)\n",
    "    print(\"NN multified: {:0.3E}\".format(idemp_error(p_nn_multi, s))) \n",
    "    print(\"Value before: {:0.3E}\".format(np.mean(np.abs(p.reshape(dim, dim) - p_nn))))\n",
    "    print(\"Value after:  {:0.3E}\".format(np.mean(np.abs(p.reshape(dim, dim) - p_nn_multi))))\n",
    "    print(\"Is nan: \" + str(np.sum(np.isnan(p_nn_multi))))\n",
    "    print(\"Is inf: \" + str(np.sum(np.isinf(p_nn_multi))))\n",
    "    print(\"Is fin: \" + str(np.sum(np.isfinite(p_nn_multi))))\n",
    "    print(\"--------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyscf.scf import hf\n",
    "from SCFInitialGuess.utilities.plotutils import prediction_scatter\n",
    "\n",
    "dim = 26\n",
    "\n",
    "\n",
    "\n",
    "iterations = []\n",
    "for i, (molecule, p) in enumerate(zip(molecules_test, P_test)):\n",
    "    \n",
    "    mol = molecule.get_pyscf_molecule()\n",
    "    \n",
    "    \n",
    "    print(\"Calculating: \" + str(i + 1) + \"/\" + str(len(molecules_test)))\n",
    "    \n",
    "\n",
    "    s_raw = hf.get_ovlp(mol)\n",
    "    s_norm = dataset.input_transformation(s_raw.reshape(1, dim**2))\n",
    "    \n",
    "    with graph_orig.as_default():\n",
    "        P_orig = network_orig.run(sess_orig, s_norm).reshape(dim, dim).astype('float64')\n",
    "        \n",
    "        P_orig_sym = (P_orig + P_orig.T) / 2\n",
    "        P_orig_idem = multi_mc_wheeny(P_orig, s_raw, n_max=5)\n",
    "        \n",
    "        # check errors\n",
    "        print(\"Accuracy (MSE):\")\n",
    "        print(\"Orig: {:0.3E}\".format(np.mean(np.abs(p.reshape(dim, dim) - P_orig)**2)))\n",
    "        print(\"Sym:  {:0.3E}\".format(np.mean(np.abs(p.reshape(dim, dim) - P_orig_sym)**2)))\n",
    "        print(\"Idem: {:0.3E}\".format(np.mean(np.abs(p.reshape(dim, dim) - P_orig_idem)**2)))\n",
    "        \n",
    "        print(\"Idempotency:\")\n",
    "        print(\"Orig: {:0.3E}\".format(idemp_error(P_orig, s_raw)))\n",
    "        print(\"Sym:  {:0.3E}\".format(idemp_error(P_orig_sym, s_raw)))\n",
    "        print(\"Idem: {:0.3E}\".format(idemp_error(P_orig_idem, s_raw)))\n",
    "\n",
    "    \n",
    "    # P_actual wi noise\n",
    "    p_noise = p.reshape(dim, dim) + np.random.randn(dim, dim) * 1e-4\n",
    "    \n",
    "    \n",
    "    iterations_molecule = []\n",
    "\n",
    "    for guess in [p_noise, P_orig, P_orig_sym, P_orig_idem]:\n",
    "        mf = hf.RHF(mol)\n",
    "        mf.diis = None\n",
    "        mf.verbose = 1\n",
    "        mf.kernel(dm0=guess)\n",
    "        iterations_molecule.append(mf.iterations)\n",
    "    \n",
    "    iterations.append(iterations_molecule)\n",
    "\n",
    "iterations = np.array(iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(iterations,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
